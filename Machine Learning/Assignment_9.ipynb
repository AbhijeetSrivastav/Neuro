{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q1. What is feature engineering, and how does it work? Explain the various aspects of feature\n",
    "engineering in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Feature engineering is a phase in data science project in which we do various tasks such as:\n",
    "- Feature selection\n",
    "- Feature transformation\n",
    "- Feature extraction\n",
    "\n",
    "\n",
    "**Working**\n",
    "\n",
    "- Imputation: treating missing values in a feature\n",
    "- Encoding: encode categorical data to numeric values\n",
    "- Outlier: detection and treating outliers\n",
    "- Selection: features which affect output of model selected\n",
    "- Transformation: transforming values of feature to get required distribution in feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2. What is feature selection, and how does it work? What is the aim of it? What are the various\n",
    "methods of function selection?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: Feature selection deals with selecting only relevant features which affect the output of the model.\n",
    "\n",
    "Various ways to feature selection are:\n",
    "- Wrapper methods\n",
    "- Filter methods\n",
    "- Embedded methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q3. Describe the function selection filter and wrapper approaches. State the pros and cons of each\n",
    "approach?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "\n",
    "**Wrapper approach**\n",
    "(forward, backward, and stepwise selection): In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "\n",
    "**Selection filter**\n",
    "(ANOVA, Pearson correlation, variance thresholding): Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q4.\n",
    "\n",
    "i. Describe the overall feature selection process.\n",
    "\n",
    "ii. Explain the key underlying principle of feature extraction using an example. What are the most\n",
    "widely used function extraction algorithms?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "- Feature selection deals with selecting only relevant features which affect the output of the model.\n",
    "\n",
    "- There are three types of feature selection:\n",
    "\n",
    "    - Wrapper methods (forward, backward, and stepwise selection): In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive.\n",
    "    - Filter methods (ANOVA, Pearson correlation, variance thresholding): Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable. The correlation is a subjective term here.\n",
    "    - Embedded methods (Lasso, Ridge, Decision Tree): Embedded methods combine the qualities’ of filter and wrapper methods. It’s implemented by algorithms that have their own built-in feature selection methods. Some of the most popular examples of these methods are LASSO and RIDGE regression which have inbuilt penalization functions to reduce overfitting. Lasso regression performs L1 regularization which adds penalty equivalent to absolute value of the magnitude of coefficients. Ridge regression performs L2 regularization which adds penalty equivalent to square of the magnitude of coefficients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q5. Describe the feature engineering process in the sense of a text categorization issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "- Text categorization problem we have classify text to some class.\n",
    "- To do so we need to extract some features from the raw text to train our model, i.e feature creation\n",
    "- Over that we have to clean the raw text which may include following steps:\n",
    "    - converting whole text to singular case\n",
    "    - removal of stop words\n",
    "    - removal of white spaces\n",
    "    - tokenization of words\n",
    "    - many more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q6. What makes cosine similarity a good metric for text categorization? A document-term matrix has\n",
    "two rows with values of (2, 3, 2, 0, 2, 3, 3, 0, 1) and (2, 1, 0, 0, 3, 2, 1, 3, 1). Find the resemblance in\n",
    "cosine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Cosine similarity is measure of similarity in the documents irrespective of their size.\n",
    "\n",
    "It is good for text categorization because if the documents are too far by euclidean distance due to their size still their is chance that they can eb oriented closer together.\n",
    "\n",
    "\n",
    "```\n",
    "Cos(x, y) = (x.y) / ||x|| * ||y||\n",
    "```\n",
    "\n",
    "so , cos(x, y) from question is: 0.675"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q7.\n",
    "\n",
    "i. What is the formula for calculating Hamming distance? Between 10001011 and 11001111,\n",
    "calculate the Hamming gap.\n",
    "\n",
    "ii. Compare the Jaccard index and similarity matching coefficient of two features with values (1, 1, 0,\n",
    "0, 1, 0, 1, 1) and (1, 1, 0, 0, 0, 1, 1, 1), respectively (1, 0, 0, 1, 1, 0, 0, 1).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "\n",
    "```\n",
    "Hamming distance(HD) is, x XOR y\n",
    "\n",
    "Jascard Index(JI) = (x intersct y) / (x + y - (x intersect y))\n",
    "```\n",
    "\n",
    "\n",
    "HD = 4\n",
    "\n",
    "JI(x,y) = (2 / 2) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q8. State what is meant by &quot;high-dimensional data set&quot;? Could you offer a few real-life examples?\n",
    "What are the difficulties in using machine learning techniques on a data set with many dimensions?\n",
    "What can be done about it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "Suppose we have a dataset `d` with `p` number of features and we take `n` samples out of it.\n",
    "\n",
    "Then if `p` > `n` then such dataset is called high dimensional dataset.\n",
    "\n",
    "Ex- ECG, EEG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q9. Make a few quick notes on:\n",
    "\n",
    "1. PCA is an acronym for Personal Computer Analysis.\n",
    "\n",
    "2. Use of vectors\n",
    "\n",
    "3. Embedded technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: \n",
    "\n",
    "- The Principal component analysis (PCA) is a technique used for identification of a smaller number of uncorrelated variables known as principal components from a larger set of data. \n",
    "- Vectors can be used to represent physical quantities which has magnitude and direction as well.\n",
    "- An embedding is a low-dimensional, learned continuous vector representation of discrete variables into which we can translate high-dimensional vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q10. Make a comparison between:\n",
    "\n",
    "1. Sequential backward exclusion vs. sequential forward selection\n",
    "\n",
    "2. Function selection methods: filter vs. wrapper\n",
    "\n",
    "3. SMC vs. Jaccard coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "\n",
    "- Sequential floating forward selection (SFFS) starts from the empty set. After each forward step, SFFS performs backward steps as long as the objective function increases. Sequential floating backward selection (SFBS) starts from the full set.\n",
    "\n",
    "- In wrapper methods, we try to use a subset of features and train a model using them. Based on the inferences that we draw from the previous model, we decide to add or remove features from your subset. The problem is essentially reduced to a search problem. These methods are usually computationally very expensive. While Filter methods are generally used as a preprocessing step. The selection of features is independent of any machine learning algorithms. Instead, features are selected on the basis of their scores in various statistical tests for their correlation with the outcome variable.\n",
    "\n",
    "- The Jaccard coefficient is a measure of the percentage of overlap between sets defined as: J(w1, w2 ) where w1 and w2 are two sets, in our case the 1-year windows of the ego networks. The Jaccard coefficient can be a value between 0 and 1, with 0 indicating no overlap and 1 complete overlap between the sets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2a8dfe095fce2b5e88c64a2c3ee084c8e0e0d70b23e7b95b1cfb538be294c5c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
